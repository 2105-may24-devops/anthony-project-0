{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\r\n",
    "    File name: DataZero.py\r\n",
    "    Author: Anthony M. Jarvis / @iamjarvi\r\n",
    "    Date created: 6/1/2021\r\n",
    "    Date last modified: 6/3/2021\r\n",
    "    Python Version: 3.9\r\n",
    "'''\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from urllib.parse import urlparse\r\n",
    "import urllib.request\r\n",
    "\r\n",
    "# Permission Check Function\r\n",
    "def url_check(input_url):\r\n",
    "    \r\n",
    "    # Get input URL string. Urlparse chunks the address into Scheme, Netloc, Path, Params, Query, Fragment.\r\n",
    "    \r\n",
    "    url = str(input_url)\r\n",
    "\r\n",
    "    parsed_url = urlparse(url)\r\n",
    "    scheme = str(parsed_url.scheme)\r\n",
    "    netloc = str(parsed_url.netloc)\r\n",
    "    path = str(parsed_url.path)\r\n",
    "\r\n",
    "    # Ammend file with 'robots.txt'. This is a file--most websites have--that list User-Agent permissions\r\n",
    "    # for scraping the data on their page.\r\n",
    "\r\n",
    "    robots_url = str(f\"{scheme}://{netloc}/robots.txt\")\r\n",
    "    \r\n",
    "    # Using urllib's request; get that robot.txt.\r\n",
    "\r\n",
    "    robots_file = urllib.request.urlopen(robots_url)\r\n",
    "      \r\n",
    "    # Decode the requested file and store it.\r\n",
    "\r\n",
    "    for line in robots_file:\r\n",
    "        decoded_file = line.decode(\"utf-8\")\r\n",
    "\r\n",
    "    # Check for Permissions for the path we're going to scrape.  \r\n",
    "    # Currently (Red = Path Not Allowed, Green = Path Allowed)\r\n",
    "    # !! More stringent checks go here !!\r\n",
    "\r\n",
    "    permission_str = f\"Disallow: {path}\"      \r\n",
    "\r\n",
    "    # Give the user a level of Risk.    \r\n",
    "    if permission_str in decoded_file:\r\n",
    "        print(\"Red\") \r\n",
    "    else:\r\n",
    "        print(\"Green\")\r\n",
    "\r\n",
    "    return url, netloc\r\n",
    "\r\n",
    "# Scraper Function - More details to come.\r\n",
    "    \r\n",
    "def url_scrape(a, b):\r\n",
    "\r\n",
    "    url = a\r\n",
    "    netloc = b\r\n",
    "\r\n",
    "    netloc.replace('.', '-')\r\n",
    "    page = requests.get(url)\r\n",
    "    soup = BeautifulSoup(page.text)\r\n",
    "\r\n",
    "    # Get the table.\r\n",
    "\r\n",
    "    table = soup.find('table')\r\n",
    "\r\n",
    "    headers = []\r\n",
    "\r\n",
    "    for i in table.find_all('th'):\r\n",
    "        title = i.text.strip()\r\n",
    "        headers.append(title)\r\n",
    "\r\n",
    "    df = pd.DataFrame(columns = headers)\r\n",
    "\r\n",
    "    for row in table.find_all('tr')[1:]:\r\n",
    "        data = row.find_all('td')\r\n",
    "        row_data = [td.text.strip() for td in data]\r\n",
    "        length = len(df)\r\n",
    "        df.loc[length] = row_data\r\n",
    "\r\n",
    "    print(\"Scraping successful!\")\r\n",
    "\r\n",
    "    # Saving the file\r\n",
    "\r\n",
    "    download = input(\"Would you like to download this data? y/n \")\r\n",
    "\r\n",
    "    if download == 'y':\r\n",
    "        import time\r\n",
    "        timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n",
    "        df.to_csv(f\"{netloc}_{timestr}\")\r\n",
    "        print('Save successful!')\r\n",
    "    else:\r\n",
    "        print(\"Here's a snapshot, then.\")\r\n",
    "        print(df)\r\n",
    "\r\n",
    "\r\n",
    "# Enter the program\r\n",
    "if __name__==\"__main__\":\r\n",
    "\r\n",
    "    print(\"Welcome to Data Zero!\\n\")\r\n",
    "    url_checked, netloc_checked = url_check(input_url=input(\"Input a url: \\n\"))\r\n",
    "\r\n",
    "    print(\"Scrape this page?\\n\")\r\n",
    "    answer = input('y/n\\n')\r\n",
    "\r\n",
    "    if answer == 'y':\r\n",
    "        url_scrape(url_checked, netloc_checked)"
   ]
  }
 ],
 "metadata": {
  "language_info": {},
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}